# @package _global_

disease: Parkinson        # Disease type. Options: [Parkinson, Schizophrenia]
data_type: harmonized     # Data type. Options: [non_harmonized, harmonized]
model_type: tabnet        # Model type. Options: [tabnet, node]
outcome: "Status"         # Which column in `data.xlsx` contains class labels

name: ${disease}_${data_type}_${model_type}

seed: 42

cv_is_split: False  # Perform cross-validation?
cv_n_splits: 5      # Number of splits in cross-validation
cv_n_repeats: 5     # Number of repeats in cross-validation

optimized_metric: "accuracy_weighted"   # Target metric in optimization process. Options: [accuracy_weighted, f1_weighted, auroc_weighted, ...]
optimized_mean: ""                      # Optimizing mean metric value across all cross-validation splits. Options: ["", cv_mean]
optimized_part: "val"                   # Which partition should be optimized? Options: [trn, val, tst]
direction: "max"

debug: False
print_config: True
ignore_warnings: True
test_after_training: True

max_epochs: 2000  # Maximum number of epochs in training process
patience: 100     # How many validation epochs of not improving until training stops

in_dim: 1000
out_dim: 2

original_work_dir: ${oc.env:PROJECT_ROOT}
base_dir: "${original_work_dir}/data/${disease}"
work_dir: "${base_dir}/${data_type}/models/${name}"
data_dir: "${base_dir}/${data_type}"

# SHAP values
is_shap: False                    # SHAP values calculation. Options: [True, False]
is_shap_save: False               # Save SHAP values to file?
shap_explainer: Kernel            # Explainer type for SHAP values. Options: for SA: [Tree, Kernel], for PL: [Deep, Kernel]
shap_bkgrd: trn                   # Background for calculating SHAP values. Options: [trn, all, tree_path_dependent]. Last option works only for GBDT models

# Plot params
num_top_features: 5
num_examples: 5

# specify here default training configuration
defaults:
  - _self_
  - override /datamodule: null
  - override /trainer: default.yaml # choose trainer from 'configs/trainer/'
  - override /model: null
  - override /callbacks: default.yaml
  - override /logger: many_loggers.yaml # set logger here or use command line (e.g. `python run.py logger=wandb`)
  - override /log_dir: default.yaml
  - override /hydra/hydra_logging: colorlog
  - override /hydra/job_logging: colorlog

datamodule:
  _target_: src.datamodules.dnam.DNAmDataModule
  feat_fn: "${data_dir}/feat.xlsx"
  label_fn: "${data_dir}/label.xlsx"
  data_fn: "${data_dir}/data.xlsx"
  outcome: ${outcome}
  batch_size: 128
  num_workers: 0
  pin_memory: False
  seed: ${seed}
  imputation: "fast_knn"
  k: 1

trainer:
  gpus: 1
  min_epochs: 1
  max_epochs: ${max_epochs}
  resume_from_checkpoint: null
  progress_bar_refresh_rate: 10

model:
  type: ${model_type}

tabnet:
  _target_: src.models.tabnet.model.TabNetModel
  task: "classification"
  loss_type: "CrossEntropyLoss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.01
  optimizer_weight_decay: 0.0
  scheduler_step_size: 30
  scheduler_gamma: 0.9
  n_d_n_a: 8
  n_steps: 3
  gamma: 1.3
  n_independent: 1
  n_shared: 2
  virtual_batch_size: 128
  mask_type: "sparsemax"

node:
  _target_: src.models.node.model.NodeModel
  task: "classification"
  loss_type: "CrossEntropyLoss"
  input_dim: ${in_dim}
  output_dim: ${out_dim}
  optimizer_lr: 0.001
  optimizer_weight_decay: 0.0
  scheduler_step_size: 30
  scheduler_gamma: 0.9
  num_trees: 1024
  num_layers: 1
  flatten_output: False
  depth: 5

callbacks:
  model_checkpoint:
    monitor: "val/${optimized_metric}_pl" # name of the logged metric which determines when model is improving
    mode: ${direction} # can be "max" or "min"
    save_top_k: 1 # save k best models (determined by above metric)
    save_last: False # additionaly always save model from last epoch
    verbose: False
    dirpath: ""
    filename: "best"
    auto_insert_metric_name: False

  early_stopping:
    monitor: "val/${optimized_metric}_pl" # name of the logged metric which determines when model is improving
    mode: ${direction} # can be "max" or "min"
    patience: ${patience} # how many epochs of not improving until training stops
    min_delta: 0 # minimum change in the monitored metric needed to qualify as an improvement
