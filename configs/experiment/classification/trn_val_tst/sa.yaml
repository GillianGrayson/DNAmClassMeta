# @package _global_

disease: Parkinson        # Disease type. Options: [Parkinson, Schizophrenia]
data_type: harmonized     # Data type. Options: [non_harmonized, harmonized]
model_type: svm      # Model type. Options: for SA: [xgboost, catboost, lightgbm, logistic_regression, svm], for PL: [tabnet, node]
outcome: "Status"         # Which column in `data.xlsx` contains class labels

name: ${disease}_${data_type}_${model_type}

seed: 42

cv_is_split: False  # Perform cross-validation?
cv_n_splits: 5      # Number of splits in cross-validation
cv_n_repeats: 5     # Number of repeats in cross-validation

optimized_metric: "accuracy_weighted"   # Target metric in optimization process. Options: [accuracy_weighted, f1_weighted, auroc_weighted, ...]
optimized_mean: ""                      # Optimizing mean metric value across all cross-validation splits. Options: ["", cv_mean]
optimized_part: "val"                   # Which partition should be optimized? Options: [trn, val, tst]
direction: "max"                        # Optimization metrics should be minimized or maximized? Options: [max, min]

debug: False
print_config: True
ignore_warnings: True
test_after_training: True

max_epochs: 2000  # Maximum number of epochs in training process
patience: 100     # How many validation epochs of not improving until training stops

in_dim: 1000
out_dim: 2

original_work_dir: ${hydra:runtime.cwd}
base_dir: "${original_work_dir}/data/${disease}"
work_dir: "${base_dir}/${data_type}/models/${name}"
data_dir: "${base_dir}/${data_type}"

# SHAP values
is_shap: False                    # SHAP values calculation. Options: [True, False]
is_shap_save: False               # Save SHAP values to file?
shap_explainer: Tree              # Explainer type for SHAP values. Options: for SA: [Tree, Kernel], for PL: [Deep, Kernel]
shap_bkgrd: tree_path_dependent   # Background for calculating SHAP values. Options: [trn, all, tree_path_dependent]. Last option works only for GBDT models

# Plot params
num_top_features: 5
num_examples: 5

# specify here default training configuration
defaults:
  - _self_
  - override /trainer: null # choose trainer from 'configs/trainer/'
  - override /model: null
  - override /datamodule: null
  - override /callbacks: none.yaml
  - override /logger: many_loggers.yaml # set logger here or use command line (e.g. `python run.py logger=wandb`)
  - override /log_dir: default.yaml
  - override /hydra/hydra_logging: colorlog
  - override /hydra/job_logging: colorlog

datamodule:
  _target_: src.datamodules.dnam.DNAmDataModule   # Instantiated datamodule
  feat_fn: "${data_dir}/feat.xlsx"                # Filename of target features file
  label_fn: "${data_dir}/label.xlsx"              # Filename of target labels file
  data_fn: "${data_dir}/data.xlsx"                # Filename of data file
  outcome: ${outcome}                             # Which column in `data_fn' contains class labels
  batch_size: 128                                 # How many samples per batch to load
  num_workers: 0                                  # How many subprocesses to use for data loading
  pin_memory: False                               # Data loader will copy Tensors into CUDA pinned memory before returning them
  seed: ${seed}                                   # Random seed
  imputation: "fast_knn"                          # Imputation method for missing values. Options: [median, mean, fast_knn, random, mice, em, mode]
  k: 1                                            # k for 'fast_knn' imputation method

# XGBoost model params
model_xgboost:
  output_dim: ${out_dim}
  booster: 'gbtree'
  learning_rate: 0.01
  max_depth: 6
  gamma: 0
  sampling_method: 'uniform'
  subsample: 1
  objective: 'multi:softprob'
  verbosity: 1
  eval_metric: 'mlogloss'
  max_epochs: ${max_epochs}
  patience: ${patience}

# CatBoost model params
model_catboost:
  output_dim: ${out_dim}
  loss_function: 'MultiClass'
  learning_rate: 0.01
  depth: 4
  min_data_in_leaf: 4
  max_leaves: 31
  task_type: 'CPU'
  verbose: 1
  max_epochs: ${max_epochs}
  patience: ${patience}

# LightGBM model params
model_lightgbm:
  output_dim: ${out_dim}
  objective: 'multiclass'
  boosting: 'gbdt'
  learning_rate: 0.01
  num_leaves: 31
  device: 'cpu'
  max_depth: -1
  min_data_in_leaf: 20
  feature_fraction: 0.9
  bagging_fraction: 0.8
  bagging_freq: 5
  verbose: 0
  metric: 'multi_logloss'
  max_epochs: ${max_epochs}
  patience: ${patience}

logistic_regression:
  penalty: "elasticnet"
  l1_ratio: 0.5
  C: 1.0
  multi_class: "multinomial"
  solver: "saga"
  max_iter: 100
  tol: 1e-4
  verbose: 0

svm:
  C: 1.0
  kernel: "rbf"
  decision_function_shape: "ovr"
  max_iter: -1
  tol: 1e-2
  verbose: 1
